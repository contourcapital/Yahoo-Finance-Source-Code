{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta, date\n",
    "import io\n",
    "import yfinance as yf\n",
    "import openpyxl\n",
    "\n",
    "Termsheet = pd.read_csv(\"C:\\\\Users\\\\dbcin\\\\OneDrive\\\\Desktop\\\\Equity File Storage\\\\EQUITY_L (1).csv\")\n",
    "df = pd.DataFrame(Termsheet)\n",
    "filter_condition = df[' SERIES'] == \"EQ\"\n",
    "Data = df[filter_condition]\n",
    "ticker_symbols = Data.SYMBOL\n",
    "\n",
    "base_folder = r\"C:\\Users\\dbcin\\OneDrive\\Desktop\\Equity File Storage\"\n",
    "folder_name = datetime.today().strftime('%d-%B-%Y')\n",
    "folder_path = os.path.join(base_folder, folder_name)\n",
    "\n",
    "#print(f\"Folder path reads as follows: '{folder_path}'\")\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok= True)\n",
    "    print(f\"Folder '{folder_name}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate start and end dates\n",
    "end_date_str = (datetime.today() + timedelta(days=1)).strftime('%d-%m-%Y')\n",
    "start_date_str = (datetime.today() - timedelta(days=500)).strftime('%d-%m-%Y')\n",
    "print(f\"Today's Date is {end_date_str}. The t-period commences on {start_date_str}\")\n",
    "\n",
    "#Converting start and end date to datetime objects\n",
    "end_date = datetime.strptime(end_date_str,'%d-%m-%Y')\n",
    "start_date = datetime.strptime(start_date_str,'%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_tickers = []\n",
    "total_files_downloaded = 0\n",
    "\n",
    "for ticker in ticker_symbols:\n",
    "    try:\n",
    "        stockdata = yf.download(f'{ticker}.NS', start = start_date, end = end_date)\n",
    "        if not stockdata.empty:\n",
    "            file_path = os.path.join(folder_path, f'{ticker}.csv')\n",
    "            stockdata.to_csv(file_path)\n",
    "            \n",
    "            print(f\"Saved data to {file_path} for {ticker}\")\n",
    "    \n",
    "            total_files_downloaded += 1\n",
    "            \n",
    "    except (ValueError, KeyError) as e:\n",
    "        print(f\"Failed to fetch the data for {ticker} due to : {e}\")\n",
    "        error_tickers.append({\"Ticker\": ticker, \"Error Type\": str(e)})\n",
    "\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"Failed to connect for {ticker} due to: {e}\")\n",
    "        error_tickers.append({\"Ticker\": ticker, \"Error Type\": \"Connection Error\"})\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        if \"NameResolutionError\" in error_message and \"query2.finance.yahoo.com\" in error_message:\n",
    "            print(f\"Failed to connect for {ticker} due to: {e}\")\n",
    "            error_tickers.append({\"Ticker\": ticker, \"Error Type\": \"Connection Error (Failed to connect)\"})\n",
    "        else:\n",
    "            print(f\"Unexpected error occured for {ticker} due to : {e}\")\n",
    "            error_tickers.append({\"Ticker\": ticker, \"Error Type\": \"Unexpected Error\"})\n",
    "\n",
    "if len(error_tickers) > 0:\n",
    "    print(\"Tickers with errors:\")\n",
    "    error_df = pd.DataFrame(error_tickers)\n",
    "    print(error_df)\n",
    "else:\n",
    "    print(\"No errors occurred for any tickers.\")\n",
    "    \n",
    "print(f\"Total number of files downloaded stand at : {total_files_downloaded} on {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_folder = r\"C:\\Users\\dbcin\\OneDrive\\Desktop\\Consolidated Stock Data\"\n",
    "output_folder = os.path.join(output_base_folder,datetime.today().strftime('%d-%B-%Y'))\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Check if the current time is before or after 12 PM\n",
    "if now.hour < 12:\n",
    "    today_date = (now.date() - datetime.timedelta(days=1)).strftime('%d-%m-%Y')\n",
    "else:\n",
    "    today_date = now.date().strftime('%d-%m-%Y')\n",
    "\n",
    "print(f\"Today's Date is = {today_date}\")\n",
    "\n",
    "output_file =  f\"Consolidated Data for {today_date}.xlsx\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"Output Folder '{output_folder}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{output_folder}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidating Close Data\n",
    "\n",
    "consolidated_data_close = pd.DataFrame()\n",
    "consolidated_data_open = pd.DataFrame()\n",
    "consolidated_data_high = pd.DataFrame()\n",
    "consolidated_data_low = pd.DataFrame()\n",
    "consolidated_volume = pd.DataFrame()\n",
    "\n",
    "#Close\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path , filename)\n",
    "            stock_data = pd.read_csv(file_path)\n",
    "        \n",
    "            close_data = stock_data[['Date','Close']]       \n",
    "            close_data.set_index(\"Date\", inplace = True)\n",
    "                    \n",
    "                    \n",
    "            stock_symbol = os.path.splitext(filename)[0]\n",
    "            close_data.rename(columns = {\"Close\": stock_symbol}, inplace = True)\n",
    "\n",
    "            consolidated_data_close = pd.concat([consolidated_data_close,close_data],axis = 1)       \n",
    "        \n",
    "#Open\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path , filename)\n",
    "            stock_data = pd.read_csv(file_path)\n",
    "        \n",
    "            open_data = stock_data[['Date','Open']]       \n",
    "            open_data.set_index(\"Date\", inplace = True)\n",
    "                    \n",
    "                    \n",
    "            stock_symbol = os.path.splitext(filename)[0]\n",
    "            open_data.rename(columns = {\"Open\": stock_symbol}, inplace = True)\n",
    "        \n",
    "            consolidated_data_open = pd.concat([consolidated_data_open,open_data],axis = 1)\n",
    "\n",
    "\n",
    "#High\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path , filename)\n",
    "            stock_data = pd.read_csv(file_path)\n",
    "        \n",
    "            high_data = stock_data[['Date','High']]       \n",
    "            high_data.set_index(\"Date\", inplace = True)\n",
    "                    \n",
    "                    \n",
    "            stock_symbol = os.path.splitext(filename)[0]\n",
    "            high_data.rename(columns = {\"High\": stock_symbol}, inplace = True)\n",
    "        \n",
    "            consolidated_data_high = pd.concat([consolidated_data_high,high_data],axis = 1)\n",
    "\n",
    "#Low\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path , filename)\n",
    "            stock_data = pd.read_csv(file_path)\n",
    "        \n",
    "            low_data = stock_data[['Date','Low']]       \n",
    "            low_data.set_index(\"Date\", inplace = True)\n",
    "                    \n",
    "                    \n",
    "            stock_symbol = os.path.splitext(filename)[0]\n",
    "            low_data.rename(columns = {\"Low\": stock_symbol}, inplace = True)\n",
    "        \n",
    "            consolidated_data_low = pd.concat([consolidated_data_low,low_data],axis = 1)\n",
    "\n",
    "\n",
    "#Volume\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path , filename)\n",
    "            stock_data = pd.read_csv(file_path)\n",
    "        \n",
    "            volume_data = stock_data[['Date','Volume']]       \n",
    "            volume_data.set_index(\"Date\", inplace = True)\n",
    "                    \n",
    "                    \n",
    "            stock_symbol = os.path.splitext(filename)[0]\n",
    "            volume_data.rename(columns = {\"Volume\": stock_symbol}, inplace = True)\n",
    "        \n",
    "            consolidated_volume = pd.concat([consolidated_volume,volume_data],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_synchronization(dataframes):\n",
    "    # Check column headers\n",
    "    first_columns = [df.columns.tolist() for df in dataframes]\n",
    "    if not all(col == first_columns[0] for col in first_columns):\n",
    "        raise ValueError(\"Column headers are not synchronized across dataframes.\")\n",
    "\n",
    "    # Check indexes (dates)\n",
    "    first_indexes = [df.index.tolist() for df in dataframes]\n",
    "    if not all(idx == first_indexes[0] for idx in first_indexes):\n",
    "        raise ValueError(\"Indexes (dates) are not synchronized across dataframes.\")\n",
    "\n",
    "dataframes = [consolidated_data_close, consolidated_data_open, consolidated_data_high, consolidated_data_low, consolidated_volume]\n",
    "\n",
    "# Check synchronization of column headers and indexes across dataframes\n",
    "try:\n",
    "    check_synchronization(dataframes)\n",
    "    print(\"Column headers and indexes are synchronized across dataframes.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronize_dates_across_dataframes(dataframes):\n",
    "    # Find the common date range across all dataframes\n",
    "    common_index = None\n",
    "    for df in dataframes:\n",
    "        if common_index is None:\n",
    "            common_index = df.index\n",
    "        else:\n",
    "            common_index = common_index.intersection(df.index)\n",
    "    \n",
    "    # Print the common date range\n",
    "    if common_index is not None:\n",
    "        print(f\"Common date range across all dataframes: {common_index.min()} to {common_index.max()}\")\n",
    "\n",
    "        # Trim all dataframes to the common date range\n",
    "        for df in dataframes:\n",
    "            df.drop(df.index.difference(common_index), inplace=True)\n",
    "        \n",
    "        # Print message indicating synchronization and updated last date\n",
    "        print(f\"Synchronized dates across dataframes. Last date is now: {common_index[-1]}\")\n",
    "    else:\n",
    "        print(\"No common date range found among the dataframes.\")\n",
    "\n",
    "dataframes = [consolidated_data_close, consolidated_data_open, consolidated_data_high, consolidated_data_low, consolidated_volume]\n",
    "\n",
    "# Synchronize dates across dataframes\n",
    "synchronize_dates_across_dataframes(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Check if the current time is before or after 12 PM\n",
    "if now.hour < 12:\n",
    "    today_date = (now.date() - datetime.timedelta(days=1)).strftime('%d-%m-%Y')\n",
    "else:\n",
    "    today_date = now.date().strftime('%d-%m-%Y')\n",
    "\n",
    "print(f\"Today's Date is = {today_date}\")\n",
    "\n",
    "\n",
    "# Function to print the common date range across all dataframes\n",
    "def print_common_date_range(dataframes):\n",
    "    common_index = None\n",
    "    for df in dataframes:\n",
    "        if common_index is None:\n",
    "            common_index = df.index\n",
    "        else:\n",
    "            common_index = common_index.intersection(df.index)\n",
    "    \n",
    "    if common_index is not None:\n",
    "        print(f\"Common date range across all dataframes: {common_index.min()} to {common_index.max()}\")\n",
    "    else:\n",
    "        print(\"No common date range found among the dataframes.\")\n",
    "\n",
    "\n",
    "# Function to synchronize dates across all dataframes and drop the last row if necessary\n",
    "def synchronize_and_check_end_date(dataframes, end_date):\n",
    "    # Find the common date range across all dataframes\n",
    "    common_index = None\n",
    "    for df in dataframes:\n",
    "        if common_index is None:\n",
    "            common_index = df.index\n",
    "        else:\n",
    "            common_index = common_index.intersection(df.index)\n",
    "    \n",
    "    # Print the common date range\n",
    "    if common_index is not None:\n",
    "        print(f\"Common date range across all dataframes: {common_index.min()} to {common_index.max()}\")\n",
    "\n",
    "        # Trim all dataframes to the common date range\n",
    "        for df in dataframes:\n",
    "            df.drop(df.index.difference(common_index), inplace=True)\n",
    "        \n",
    "        # Check if last date matches the end date\n",
    "        last_date = common_index[-1]\n",
    "        if last_date != end_date:\n",
    "            print(f\"Last date {last_date} does not match the expected end date {end_date}. Dropping last row from all dataframes.\")\n",
    "            \n",
    "            # Store the last row for potential reinsertion\n",
    "            last_rows = {i: df.loc[last_date].copy() for i, df in enumerate(dataframes)}\n",
    "\n",
    "            # Drop the last row from all dataframes\n",
    "            for df in dataframes:\n",
    "                df.drop(last_date, inplace=True)\n",
    "            \n",
    "            # Re-check and print updated last date\n",
    "            common_index = None\n",
    "            for df in dataframes:\n",
    "                if common_index is None:\n",
    "                    common_index = df.index\n",
    "                else:\n",
    "                    common_index = common_index.intersection(df.index)\n",
    "            \n",
    "            if common_index is not None:\n",
    "                last_date = common_index[-1]\n",
    "                print(f\"New last date after dropping row: {last_date}\")\n",
    "\n",
    "            # Ask user if they want to reinsert the dropped rows\n",
    "                user_input = input(\"Do you want to reinsert the dropped rows? (yes/no): \").strip().lower()\n",
    "                if user_input == 'yes':\n",
    "                    for i, df in enumerate(dataframes):\n",
    "                        df.loc[last_rows[i].name] = last_rows[i]\n",
    "                    print(\"Dropped rows have been reinserted.\")\n",
    "                else:\n",
    "                    print(\"Chosen not to re-insert dropped rows.\")\n",
    "                print_common_date_range(dataframes)\n",
    "            else:\n",
    "                print(\"No common date range found among the dataframes after dropping row.\")\n",
    "        else:\n",
    "            print(f\"Last date matches the expected end date {end_date}. Proceeding with data as is.\")\n",
    "    else:\n",
    "        print(\"No common date range found among the dataframes.\")\n",
    "\n",
    "    return dataframes, last_date\n",
    "\n",
    "# Assuming these dataframes are defined somewhere before this point\n",
    "dataframes = [consolidated_data_close, consolidated_data_open, consolidated_data_high, consolidated_data_low, consolidated_volume]\n",
    "\n",
    "# Define your end date here (replace with actual end date logic)\n",
    "end_date = today_date\n",
    "\n",
    "# Synchronize dates across dataframes and check end date\n",
    "dataframes, last_date = synchronize_and_check_end_date(dataframes, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(output_folder,output_file)\n",
    "\n",
    "last_filled_date = consolidated_data_close.index[-1]\n",
    "\n",
    "print(f\"The last filled date is: {last_filled_date}\")\n",
    "\n",
    "empty_stocks = consolidated_data_close.columns[consolidated_data_close.loc[last_filled_date].isna()]\n",
    "\n",
    "if not empty_stocks.empty:\n",
    "    print(f\"Number of stocks with empty values = {len(empty_stocks)}\")\n",
    "    print(f\"Stocks with empty values on {last_filled_date}:\")\n",
    "    for stock_symbol in empty_stocks:\n",
    "        print(stock_symbol)\n",
    "else:\n",
    "    print(f\"No stocks with empty values on {last_filled_date}.\")\n",
    "\n",
    "#Writing the file to excel\n",
    "\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "        consolidated_data_close.to_excel(writer, sheet_name = \"Close\", index=True)\n",
    "        consolidated_data_open.to_excel(writer, sheet_name = \"Open\", index=True)\n",
    "        consolidated_data_high.to_excel(writer, sheet_name = \"High\", index=True)\n",
    "        consolidated_data_low.to_excel(writer, sheet_name = \"Low\", index=True)\n",
    "        consolidated_volume.to_excel(writer, sheet_name = \"Volume\", index=True)\n",
    "    \n",
    "print(f\"Consolidated data stored in {output_path}.\\nGet to work now. \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
